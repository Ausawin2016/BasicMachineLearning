{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 1000. # timesteps to observe before training\n",
    "EXPLORE = 2000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 1000 # number of previous transitions to remember\n",
    "BATCH = 128 # size of minibatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "W_fc1 = weight_variable([1600, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "\n",
    "W_fc2 = weight_variable([512, ACTIONS])\n",
    "b_fc2 = bias_variable([ACTIONS])\n",
    "\n",
    "\n",
    "# input layer\n",
    "s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "# hidden layers\n",
    "h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1) ### ===> 80x80x4 conv 4 ==> 20x20x32\n",
    "h_pool1 = max_pool_2x2(h_conv1)                       ### ===> 20x20x32 maxpool => 10x10x64\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2) ###=====10x10 conv2 ===>5x5x64\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3) ## 5x5 conv 1 padsame =>5x5x64\n",
    "#h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "#h_pool3_flat = tf.reshape(h_pool3, [-1, 256]) \n",
    "h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])               ##5x5x64 flaten =>1600\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# readout layer\n",
    "readout = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the cost function\n",
    "a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "cost = tf.reduce_mean(tf.square(y - readout_action)) #rms root mean square for cost function\n",
    "train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "# open up a game state to communicate with emulator\n",
    "game_state = game.GameState()\n",
    "\n",
    "# store the previous observations in replay memory\n",
    "D = deque()\n",
    "\n",
    "# saving and loading networks\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "#if checkpoint and checkpoint.model_checkpoint_path:\n",
    "#    saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "#    print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "#else:\n",
    "#    print(\"Could not find old network weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2022ad40828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 4)\n",
      "Test readout : \n",
      "[ 0.04237654  0.01803758]\n",
      "Test cost : 0.4594\n"
     ]
    }
   ],
   "source": [
    "def prepare_img(x_t):\n",
    "    x_t = tf.image.rgb_to_grayscale(x_t)\n",
    "    x_t = tf.image.resize_images(x_t,[80,80])\n",
    "    return x_t.eval()\n",
    "\n",
    "########### TEST Play ################\n",
    "# get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "do_nothing = np.zeros(ACTIONS)\n",
    "do_nothing[0] = 1\n",
    "x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "x_t = prepare_img(x_t)\n",
    "######################################\n",
    "# show image\n",
    "imtest = np.array(x_t)\n",
    "plt.imshow(np.reshape(x_t,[80,80]).T,cmap=cm.Greys)\n",
    "plt.show()\n",
    "#####################################\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=2).reshape([80,80,4])\n",
    "print(s_t.shape)\n",
    "############# TEST FEEDING ##########\n",
    "read = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "print(\"Test readout : \")\n",
    "print(read)\n",
    "a_t = np.zeros([ACTIONS])\n",
    "a_t[0] = 1\n",
    "print(\"Test cost : %.4f\" % cost.eval(feed_dict={s : [s_t], a : [a_t], y: a_t}))\n",
    "#####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stage(s_t,a_t):\n",
    "    # run the selected action and observe next state and reward\n",
    "    x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "    x_t1 = prepare_img(x_t1_colored)\n",
    "    x_t1 = np.reshape(x_t1, (80, 80, 1))    \n",
    "    #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
    "    s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "    return s_t1, r_t, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_action(epsilon,readout_t):\n",
    "    a_t = np.zeros([ACTIONS])\n",
    "    action_index = 0\n",
    "    if random.random() <= epsilon:\n",
    "        print(\"----------Random Action----------\")\n",
    "        action_index = random.randrange(ACTIONS)\n",
    "        a_t[random.randrange(ACTIONS)] = 1\n",
    "    else:\n",
    "        action_index = np.argmax(readout_t)\n",
    "        a_t[action_index] = 1\n",
    "    #maybe skip some frame with a_t[0] = 1 # do nothing\n",
    "    return a_t,action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 100 max readout 0.0461\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 200 max readout 0.0418\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 300 max readout 0.0431\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 400 max readout 0.0429\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 500 max readout 0.0464\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 600 max readout 0.0492\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 700 max readout 0.0477\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 800 max readout 0.0438\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 900 max readout 0.0365\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "----------Random Action----------\n",
      "TRY PLAY and RECORD : 1000 max readout 0.0465\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "epsilon = INITIAL_EPSILON\n",
    "rp = 0\n",
    "while len(D) < REPLAY_MEMORY:\n",
    "    \n",
    "    # choose an action epsilon greedily\n",
    "    readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "    a_t,action_index = play_action(epsilon,readout_t)\n",
    "    \n",
    "    ######### play ! ########\n",
    "    s_t1,r_t,terminal = get_stage(s_t,a_t)\n",
    "    \n",
    "    # store the transition in D\n",
    "    D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "    \n",
    "    s_t = s_t1\n",
    "    rp += 1\n",
    "    if(rp % 100 == 0):\n",
    "        print(\"TRY PLAY and RECORD : %d max readout %.4f\" % (rp,np.max(readout_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Random Action----------\n",
      "TIMESTEP 1003 / STATE explore / EPSILON 0.09999985015000001 / ACTION 1 / REWARD 0.1 / Q_MAX 3.340925e-02\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "# choose an action epsilon greedily\n",
    "readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "a_t,action_index = play_action(epsilon,readout_t)\n",
    "\n",
    "# scale down epsilon\n",
    "if epsilon > FINAL_EPSILON:\n",
    "    epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "# run the selected action and observe next state and reward\n",
    "s_t1,r_t,terminal = get_stage(s_t,a_t)\n",
    "# store the transition in D\n",
    "D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "\n",
    "if len(D) > REPLAY_MEMORY:\n",
    "    D.popleft()\n",
    "# sample a minibatch to train on\n",
    "minibatch = random.sample(D, BATCH)\n",
    "\n",
    "# get the batch variables\n",
    "s_j_batch = [d[0] for d in minibatch]\n",
    "a_batch = [d[1] for d in minibatch]\n",
    "r_batch = [d[2] for d in minibatch]\n",
    "s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "y_batch = []\n",
    "readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "for i in range(0, len(minibatch)):\n",
    "    terminal = minibatch[i][4]\n",
    "    # if terminal, only equals reward\n",
    "    if terminal:\n",
    "        y_batch.append(r_batch[i])\n",
    "    else:\n",
    "        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "# perform gradient step\n",
    "train_step.run(feed_dict = {\n",
    "    y : y_batch,\n",
    "    a : a_batch,\n",
    "    s : s_j_batch}\n",
    ")\n",
    "\n",
    "# update the old values\n",
    "s_t = s_t1\n",
    "rp += 1\n",
    "\n",
    "# save progress every 10000 iterations\n",
    "if rp % 10000 == 0:\n",
    "    saver.save(sess, 'saved_networks/dqn', global_step = rp)\n",
    "if rp % 1 == 0:\n",
    "    print(\"TIMESTEP\", rp, \"/ STATE explore / EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "        \"/ Q_MAX %e\" % np.max(readout_t))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 3",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
